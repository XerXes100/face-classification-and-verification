{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBITN0M_LKds"
      },
      "source": [
        "# Face Recognition and Verification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NH4P-HzLRQs"
      },
      "source": [
        "*   Face Recognition: Writing my own CNN model to tackle the problem of classification, consisting of 7001 identities\n",
        "*   Face Verification: Using the model trained for classification to evaluate the quality of its feature embeddings, by comparing the similarity of known and unknown identities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdoDIKWOMF59"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jza7lwiScUhb"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi # to see what GPU you have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTxfd_nqFnL9"
      },
      "outputs": [],
      "source": [
        "!pip install wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwLEd0gdPbSc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "import torchvision #This library is used for image-based operations (Augmentations)\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import glob\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scOnMklwWBY6"
      },
      "source": [
        "# Download Data from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BksgPdkQwwb"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"\",\"key\":\"\"}')\n",
        "    # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oFjaJTaRjT7"
      },
      "outputs": [],
      "source": [
        "!mkdir '/content/data'\n",
        "\n",
        "!kaggle competitions download -c 11-785-f23-hw2p2-classification\n",
        "!unzip -qo '11-785-f23-hw2p2-classification.zip' -d '/content/data'\n",
        "\n",
        "!kaggle competitions download -c 11-785-f23-hw2p2-verification\n",
        "!unzip -qo '11-785-f23-hw2p2-verification.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O68hT27SXClj"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApEuUzNLZ9ys"
      },
      "source": [
        "I started with batch_size of 20, then 32, then 45 and then 64. Setting the initial batch_sizes were giving me error in the num_workers part of the data loader so I thought it is a problem with increased batch_size and hence I kept it but then eventually since the error was persistent, I commented out that line and increased the batch_size to 64. For Learning Rate, I started with 0.2 and then I moved on to 0.15 and then moved to 0.12 after noticing gradual improvment in the prior 'lr' values. For the number of epochs, I initially kept it at around 30-40, but then I increased it to 150 because I was not able to reach higher accuracy on each model run and the model hadn't even plateaued till the last epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7qpMxG0XCJz"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'batch_size': 64,\n",
        "    'lr': 0.12,\n",
        "    'epochs': 150,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSeiKHYrM-6b"
      },
      "source": [
        "# Classification Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWu6zoQdaf8b"
      },
      "source": [
        "My project teammate tried out the following transforms and reached a very good accuracy so I built up on this while changing other parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmRX5omaNDEZ"
      },
      "outputs": [],
      "source": [
        "DATA_DIR    = '/content/data/11-785-f23-hw2p2-classification/'# TODO: Path where you have downloaded the data\n",
        "TRAIN_DIR   = os.path.join(DATA_DIR, \"train\")\n",
        "VAL_DIR     = os.path.join(DATA_DIR, \"dev\")\n",
        "TEST_DIR    = os.path.join(DATA_DIR, \"test\")\n",
        "\n",
        "# Transforms using torchvision - Refer https://pytorch.org/vision/stable/transforms.html\n",
        "\n",
        "train_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomRotation(20),\n",
        "    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "\n",
        "])# Implementing the right train transforms/augmentation methods is key to improving performance.\n",
        "\n",
        "valid_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset   = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms)\n",
        "valid_dataset   = torchvision.datasets.ImageFolder(VAL_DIR, transform= valid_transforms)\n",
        "# NO data augmentation on the validation set.\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_dataset,\n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = True,\n",
        "    # num_workers = 4,\n",
        "    pin_memory  = True\n",
        ")\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = valid_dataset,\n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = False,\n",
        "    # num_workers = 4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqSR063BGE2e"
      },
      "outputs": [],
      "source": [
        "# You can do this with ImageFolder as well, but it requires some tweaking\n",
        "class ClassificationTestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir   = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in the test directory\n",
        "        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVLB41KtGC2o"
      },
      "outputs": [],
      "source": [
        "test_dataset = ClassificationTestDataset(TEST_DIR, transforms = valid_transforms) #Why are we using val_transforms for Test Data?\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = config['batch_size'],\n",
        "    shuffle = False,\n",
        "    drop_last = False,\n",
        "    # num_workers = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4t8eU9gY0Jy"
      },
      "outputs": [],
      "source": [
        "print(\"Number of classes    : \", len(train_dataset.classes))\n",
        "print(\"No. of train images  : \", train_dataset.__len__())\n",
        "print(\"Shape of image       : \", train_dataset[0][0].shape)\n",
        "print(\"Batch size           : \", config['batch_size'])\n",
        "print(\"Train batches        : \", train_loader.__len__())\n",
        "print(\"Val batches          : \", valid_loader.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs2Xw_tl0IQ8"
      },
      "source": [
        "## Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIoRUzCbz85y"
      },
      "outputs": [],
      "source": [
        "# Visualize a few images in the dataset\n",
        "# You can write your own code, and you don't need to understand the code\n",
        "# It is highly recommended that you visualize your data augmentation as sanity check\n",
        "\n",
        "r, c    = [5, 5]\n",
        "fig, ax = plt.subplots(r, c, figsize= (15, 15))\n",
        "\n",
        "k       = 0\n",
        "dtl     = torch.utils.data.DataLoader(\n",
        "    dataset     = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms), # dont wanna see the images with transforms\n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = True,\n",
        ")\n",
        "\n",
        "for data in dtl:\n",
        "    x, y = data\n",
        "\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            img = x[k].numpy().transpose(1, 2, 0)\n",
        "            ax[i, j].imshow(img)\n",
        "            ax[i, j].axis('off')\n",
        "            k+=1\n",
        "    break\n",
        "\n",
        "del dtl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yJtIT-ViALr"
      },
      "source": [
        "I initially made submissions by training my model using Convolution layers and tried out a lot of different combinations and modifications. Although, I could  pass the low cutoff and the medium cutoff just barely (around 84.5%), I still had to cross medium and reach high cutoff. Hence, I had to shift from Convolution to ResNet and implementation of ResNet itself helped me reach my model's accuracy quite close to the high cutoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny-mh_ocWIJR"
      },
      "outputs": [],
      "source": [
        "# class Network(torch.nn.Module):\n",
        "#     \"\"\"\n",
        "#     The Very Low early deadline architecture is a 5-layer CNN. Keep in mind the parameter limit is 21M.\n",
        "\n",
        "#     The first Conv layer has 64 channels, kernel size 7, and stride 4.\n",
        "#     The next three have 128, 256, 512 and 1024 channels. Each have kernel size 3 and stride 2.\n",
        "\n",
        "#     Think about strided convolutions from the lecture, as convolutioin with stride= 1 and downsampling.\n",
        "#     For stride 1 convolution, what padding do you need for preserving the spatial resolution?\n",
        "#     (Hint => padding = kernel_size // 2) - Why?)\n",
        "\n",
        "#     Each Conv layer is accompanied by a Batchnorm and ReLU layer.\n",
        "#     Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1. Use AdaptiveAvgPool2d.\n",
        "#     Then, remove (Flatten?) these trivial 1x1 dimensions away.\n",
        "#     Look through https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "#     TODO: Fill out the model definition below!\n",
        "\n",
        "#     Why does a very simple network have 4 convolutions?\n",
        "#     Input images are 224x224. Note that each of these convolutions downsample.\n",
        "#     Downsampling 2x effectively doubles the receptive field, increasing the spatial\n",
        "#     region each pixel extracts features from. Downsampling 32x is standard\n",
        "#     for most image models.\n",
        "\n",
        "#     Why does a very simple network have high channel sizes?\n",
        "#     Every time you downsample 2x, you do 4x less computation (at same channel size).\n",
        "#     To maintain the same level of computation, you 2x increase # of channels, which\n",
        "#     increases computation by 4x. So, balances out to same computation.\n",
        "#     Another intuition is - as you downsample, you lose spatial information. We want\n",
        "#     to preserve some of it in the channel dimension.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, num_classes=7001):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.backbone = torch.nn.Sequential(\n",
        "\n",
        "#             # TODO\n",
        "#             # torch.nn.Conv2d(3, 64, kernel_size=7, stride=4, padding=3),\n",
        "#             # torch.nn.BatchNorm2d(64),\n",
        "#             # torch.nn.ReLU(),\n",
        "#             # torch.nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "#             # torch.nn.BatchNorm2d(128),\n",
        "#             # torch.nn.ReLU(),\n",
        "#             # torch.nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
        "#             # torch.nn.BatchNorm2d(256),\n",
        "#             # torch.nn.ReLU(),\n",
        "#             # torch.nn.Conv2d(256, 256, kernel_size=5, stride=2, padding=2),\n",
        "#             # torch.nn.BatchNorm2d(256),\n",
        "#             # torch.nn.ReLU(),\n",
        "#             # torch.nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             # torch.nn.BatchNorm2d(512),\n",
        "#             # torch.nn.ReLU(),\n",
        "#             # torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             # torch.nn.BatchNorm2d(512),\n",
        "#             # torch.nn.ReLU(),\n",
        "#             # torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             # torch.nn.BatchNorm2d(512),\n",
        "#             # torch.nn.ReLU(),\n",
        "#             # torch.nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1),\n",
        "#             # torch.nn.BatchNorm2d(1024),\n",
        "#             # torch.nn.ReLU(),\n",
        "#             # torch.nn.AdaptiveAvgPool2d(1),\n",
        "#             # torch.nn.Flatten()\n",
        "\n",
        "#             #----------------------------------------------------------------\n",
        "\n",
        "#             # torch.nn.Conv2d(3, 64, kernel_size=7, stride=4, padding=3),\n",
        "#             # torch.nn.BatchNorm2d(64),\n",
        "#             # torch.nn.ReLU(),\n",
        "\n",
        "#             # torch.nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "#             # torch.nn.BatchNorm2d(128),\n",
        "#             # torch.nn.ReLU(),\n",
        "\n",
        "#             # torch.nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "#             # torch.nn.BatchNorm2d(256),\n",
        "#             # torch.nn.ReLU(),\n",
        "\n",
        "#             # # torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#             # # torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "#             # # torch.nn.BatchNorm2d(256),\n",
        "#             # # torch.nn.ReLU(),\n",
        "\n",
        "#             # torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#             # torch.nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             # torch.nn.BatchNorm2d(512),\n",
        "#             # torch.nn.ReLU(),\n",
        "\n",
        "#             # torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             # torch.nn.BatchNorm2d(512),\n",
        "#             # torch.nn.ReLU(),\n",
        "\n",
        "#             # torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#             # torch.nn.Conv2d(512, 512, kernel_size=5, stride=2, padding=2),\n",
        "#             # torch.nn.BatchNorm2d(512),\n",
        "#             # torch.nn.ReLU(),\n",
        "\n",
        "#             # torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             # torch.nn.BatchNorm2d(512),\n",
        "#             # torch.nn.ReLU(),\n",
        "\n",
        "#             # torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#             # torch.nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "#             # torch.nn.BatchNorm2d(1024),\n",
        "#             # torch.nn.ReLU(),\n",
        "\n",
        "#             # torch.nn.AdaptiveAvgPool2d(1),\n",
        "#             # torch.nn.Flatten()\n",
        "\n",
        "#             #-----------------------------------------------------------\n",
        "\n",
        "#             torch.nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),\n",
        "#             torch.nn.BatchNorm2d(64),\n",
        "#             torch.nn.ReLU(),\n",
        "\n",
        "#             torch.nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
        "#             torch.nn.BatchNorm2d(128),\n",
        "#             torch.nn.ReLU(),\n",
        "\n",
        "#             torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#             torch.nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "#             torch.nn.BatchNorm2d(256),\n",
        "#             torch.nn.ReLU(),\n",
        "\n",
        "#             torch.nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             torch.nn.BatchNorm2d(512),\n",
        "#             torch.nn.ReLU(),\n",
        "\n",
        "#             torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#             torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             torch.nn.BatchNorm2d(512),\n",
        "#             torch.nn.ReLU(),\n",
        "\n",
        "#             torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             torch.nn.BatchNorm2d(512),\n",
        "#             torch.nn.ReLU(),\n",
        "\n",
        "#             torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#             torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             torch.nn.BatchNorm2d(512),\n",
        "#             torch.nn.ReLU(),\n",
        "\n",
        "#             torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#             torch.nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "#             torch.nn.BatchNorm2d(1024),\n",
        "#             torch.nn.ReLU(),\n",
        "\n",
        "#             torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#             torch.nn.AdaptiveAvgPool2d((1,1)),\n",
        "#             torch.nn.Flatten(),\n",
        "\n",
        "#             # -----------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#             )\n",
        "\n",
        "#         self.cls_layer = torch.nn.Linear(1024, num_classes)\n",
        "\n",
        "#     def forward(self, x, return_feats=False):\n",
        "#         \"\"\"\n",
        "#         What is return_feats? It essentially returns the second-to-last-layer\n",
        "#         features of a given image. It's a \"feature encoding\" of the input image,\n",
        "#         and you can use it for the verification task. You would use the outputs\n",
        "#         of the final classification layer for the classification task.\n",
        "\n",
        "#         You might also find that the classification outputs are sometimes better\n",
        "#         for verification too - try both.\n",
        "#         \"\"\"\n",
        "#         feats = self.backbone(x)\n",
        "#         out = self.cls_layer(feats)\n",
        "\n",
        "#         if return_feats:\n",
        "#             return feats\n",
        "#         else:\n",
        "#             return out\n",
        "\n",
        "# model = Network().to(DEVICE)\n",
        "# summary(model, (3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LDfFnz5jDuH"
      },
      "source": [
        "For implementing ResNet, I made a lot of changes in the architecture to keep the paramater limit in place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsgOihdLOrRJ"
      },
      "outputs": [],
      "source": [
        "class rsnet_block(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
        "        super(rsnet_block, self).__init__()\n",
        "        self.convolution = torch.nn.Sequential(\n",
        "                        torch.nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
        "                        torch.nn.BatchNorm2d(out_channels),\n",
        "                        torch.nn.ReLU(),\n",
        "                        torch.nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
        "                        torch.nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        self.downsample = downsample\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, input):\n",
        "        res = input\n",
        "        out = self.convolution(input)\n",
        "        if self.downsample:\n",
        "            res = self.downsample(input)\n",
        "        out += res\n",
        "        out = self.relu(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haVtZKOOpSnH"
      },
      "outputs": [],
      "source": [
        "class ResNet(torch.nn.Module):\n",
        "    def __init__(self, block, layers, num_classes = 7001):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inp = 64\n",
        "        self.conv1 = torch.nn.Sequential(\n",
        "                    torch.nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
        "                    torch.nn.BatchNorm2d(64),\n",
        "                    torch.nn.ReLU()\n",
        "        )\n",
        "        self.maxpool = torch.nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.layer0 = self.layer_add(block, 64, layers[0], stride = 1)\n",
        "        self.layer1 = self.layer_add(block, 128, layers[1], stride = 2)\n",
        "        self.layer2 = self.layer_add(block, 256, layers[2], stride = 2)\n",
        "        self.layer3 = self.layer_add(block, 512, layers[3], stride = 2)\n",
        "        self.avgpool = torch.nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = torch.nn.Linear(512, num_classes)\n",
        "\n",
        "    def layer_add(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inp != planes:\n",
        "\n",
        "            downsample = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(self.inp, planes, kernel_size=1, stride=stride),\n",
        "                torch.nn.BatchNorm2d(planes),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inp, planes, stride, downsample))\n",
        "        self.inp = planes\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inp, planes))\n",
        "\n",
        "        return torch.nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x, return_feats = False):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # feats = self.backbone(x)\n",
        "        # out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return x\n",
        "        else:\n",
        "            return self.fc(x)\n",
        "\n",
        "model = ResNet(rsnet_block, [3, 2, 3, 3]).to(DEVICE)\n",
        "summary(model, (3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZCn0qHuZRKj"
      },
      "source": [
        "# Setup everything for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YORPGJavjM-n"
      },
      "source": [
        "For optimizers, I tried out a several of them (like Adam, AdamW, AdamGrad) but after lot of iterations and model running, I figured out that SGD was giving me the highest accuracy improvement over each epoch, amongst all of them.\n",
        "\n",
        "For schedulers too I tried 2 of them which gave me very good accuracies in HW1P2 and hence I tried with ExponentialLR and StepLR. There wasn't a very big impact caused by switching them, in the model parameters and statistics, but I stuck to StepLR while modifying the steps at each model run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UowI9OcUYPjP"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss() # TODO: What loss do you need for a multi class classification problem?\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=1e-4)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr= config['lr'])\n",
        "# optimizer = torch.optim.Adagrad(model.parameters(), lr= config['lr'], weight_decay=1e-4)\n",
        "\n",
        "# You can try ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzM11HtcboYv"
      },
      "source": [
        "# Let's train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgSw6iJJavBZ"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Progress Bar\n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss  = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it!\n",
        "            outputs = model(images)\n",
        "            loss    = criterion(outputs, labels)\n",
        "\n",
        "        # Update no. of correct predictions & loss as we iterate\n",
        "        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss      += float(loss.item())\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct = num_correct,\n",
        "            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr']))\n",
        "        )\n",
        "\n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update()\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss  = float(total_loss / len(dataloader))\n",
        "\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5V2UdnpdEoK"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader, criterion):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
        "\n",
        "    num_correct = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        # Move images to device\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct)\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmotca6pcLLY"
      },
      "outputs": [],
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBgKGkXLrdJ"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix62_BkaLr_D"
      },
      "outputs": [],
      "source": [
        "wandb.login(key=\"\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG0vmsmbRYEi"
      },
      "outputs": [],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    # name = \"early-submission\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw2p2-ablations\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkRw1FvLqYe"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPReucn-0Gj8"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('/content/checkpoint.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "val_acc = checkpoint['val_acc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqWO8Edb0BK2"
      },
      "outputs": [],
      "source": [
        "best_valacc = 0.0\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_acc, train_loss = train(model, train_loader, optimizer, criterion)\n",
        "\n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        config['epochs'],\n",
        "        train_acc,\n",
        "        train_loss,\n",
        "        curr_lr))\n",
        "\n",
        "    val_acc, val_loss = validate(model, valid_loader, criterion)\n",
        "\n",
        "    # if epoch%2 == 1:\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n",
        "\n",
        "    wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc,\n",
        "               'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n",
        "\n",
        "    # #Save model in drive location if val_acc is better than best recorded val_acc\n",
        "    if val_acc >= best_valacc:\n",
        "      #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n",
        "      print(\"Saving model\")\n",
        "      torch.save({'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optimizer.state_dict(),\n",
        "                  'scheduler_state_dict':scheduler.state_dict(),\n",
        "                  'val_acc': val_acc,\n",
        "                  'epoch': epoch}, './checkpoint.pth')\n",
        "      best_valacc = val_acc\n",
        "      wandb.save('checkpoint.pth')\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_N6kJrFsahy"
      },
      "outputs": [],
      "source": [
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpgCHImRkYQW"
      },
      "source": [
        "# Classification Task: Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1BlEjkwKmyv"
      },
      "outputs": [],
      "source": [
        "# Loading the model\n",
        "\n",
        "checkpoint = torch.load('/content/checkpoint.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "val_acc = checkpoint['val_acc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2WQEUjXkWvo"
      },
      "outputs": [],
      "source": [
        "def test(model,dataloader):\n",
        "\n",
        "  model.eval()\n",
        "  batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "  test_results = []\n",
        "\n",
        "  for i, (images) in enumerate(dataloader):\n",
        "      # TODO: Finish predicting on the test set.\n",
        "      images = images.to(DEVICE)\n",
        "\n",
        "      with torch.inference_mode():\n",
        "        outputs = model(images)\n",
        "\n",
        "      outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()\n",
        "      test_results.extend(outputs)\n",
        "\n",
        "      batch_bar.update()\n",
        "\n",
        "  batch_bar.close()\n",
        "  return test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7R1lcCAzULc"
      },
      "outputs": [],
      "source": [
        "test_results = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqfUzwS2L1gx"
      },
      "source": [
        "## Generate csv to submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vob9a2-HkW_V"
      },
      "outputs": [],
      "source": [
        "with open(\"classification_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(test_dataset)):\n",
        "        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", test_results[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnRUN53CZMTf"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions submit -c 11-785-f23-hw2p2-classification -f classification_submission.csv -m \"submission\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WYgUjJzUiGU"
      },
      "source": [
        "# Verification Task: Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoBFFF8-Lpvj"
      },
      "source": [
        "The verification task consists of the following generalized scenario:\n",
        "- You are given X unknown identitites\n",
        "- You are given Y known identitites\n",
        "- Your goal is to match X unknown identities to Y known identities.\n",
        "\n",
        "We have given you a verification dataset, that consists of 960 known identities, and 1080 unknown identities. The 1080 unknown identities are split into dev (360) and test (720). Your goal is to compare the unknown identities to the 1080 known identities and assign an identity to each image from the set of unknown identities. Some unknown identities do not have correspondence in known identities, you also need to identify these and label them with a special label n000000.\n",
        "\n",
        "Your will use/finetune your model trained for classification to compare images between known and unknown identities using a similarity metric and assign labels to the unknown identities.\n",
        "\n",
        "This will judge your model's performance in terms of the quality of embeddings/features it generates on images/faces it has never seen during training for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9aY5o-suWdn"
      },
      "outputs": [],
      "source": [
        "# This obtains the list of known identities from the known folder\n",
        "known_regex = \"/content/data/11-785-f23-hw2p2-verification/known/*/*\"\n",
        "known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))]\n",
        "\n",
        "# Obtain a list of images from unknown folders\n",
        "unknown_dev_regex = \"/content/data/11-785-f23-hw2p2-verification/unknown_dev/*\"\n",
        "unknown_test_regex = \"/content/data/11-785-f23-hw2p2-verification/unknown_test/*\"\n",
        "\n",
        "unknown_dev_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_dev_regex)))]\n",
        "unknown_test_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_test_regex)))]\n",
        "known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]\n",
        "\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()])\n",
        "\n",
        "unknown_dev_images = torch.stack([transforms(x) for x in unknown_dev_images])\n",
        "unknown_test_images = torch.stack([transforms(x) for x in unknown_test_images])\n",
        "known_images  = torch.stack([transforms(y) for y in known_images ])\n",
        "\n",
        "# You can use other similarity metrics like Euclidean Distance if you wish\n",
        "similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk1LS0BRxFHM"
      },
      "outputs": [],
      "source": [
        "def eval_verification(unknown_images, known_images, model, similarity, batch_size= config['batch_size'], mode='val'):\n",
        "\n",
        "    unknown_feats, known_feats = [], []\n",
        "\n",
        "    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "    model.eval()\n",
        "\n",
        "    # We load the images as batches for memory optimization and avoiding CUDA OOM errors\n",
        "    for i in range(0, unknown_images.shape[0], batch_size):\n",
        "        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size\n",
        "\n",
        "        with torch.no_grad():\n",
        "            unknown_feat = model(unknown_batch.float().to(DEVICE), return_feats=True) #Get features from model\n",
        "        unknown_feats.append(unknown_feat)\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "\n",
        "    for i in range(0, known_images.shape[0], batch_size):\n",
        "        known_batch = known_images[i:i+batch_size]\n",
        "        with torch.no_grad():\n",
        "              known_feat = model(known_batch.float().to(DEVICE), return_feats=True)\n",
        "\n",
        "        known_feats.append(known_feat)\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    # Concatenate all the batches\n",
        "    unknown_feats = torch.cat(unknown_feats, dim=0)\n",
        "    known_feats = torch.cat(known_feats, dim=0)\n",
        "\n",
        "    similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])\n",
        "\n",
        "    max_similarity_values, predictions = similarity_values.max(0)\n",
        "    max_similarity_values, predictions = max_similarity_values.cpu().numpy(), predictions.cpu().numpy()\n",
        "\n",
        "    threshold = 0.6 # Choose a proper threshold\n",
        "    NO_CORRESPONDENCE_LABEL = 'n000000'\n",
        "    pred_id_strings = []\n",
        "    for idx, prediction in enumerate(predictions):\n",
        "        if max_similarity_values[idx] < threshold:\n",
        "            pred_id_strings.append(NO_CORRESPONDENCE_LABEL)\n",
        "        else:\n",
        "            pred_id_strings.append(known_paths[prediction])\n",
        "\n",
        "    if mode == 'val':\n",
        "      true_ids = pd.read_csv('/content/data/11-785-f23-hw2p2-verification/verification_dev.csv')['label'].tolist()\n",
        "      accuracy = accuracy_score(pred_id_strings, true_ids)\n",
        "      print(\"Verification Accuracy = {}\".format(accuracy))\n",
        "\n",
        "    return pred_id_strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ52nuvukFdD"
      },
      "source": [
        "I calculated the accuracies with a lot of thresholds and made a lot of iterations with different combinations. However, I found the accuracy to be the highest at 0.6. I even tried putting in 0.595 and 0.605, but those two gave me lesser accuracy then 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMC7FacaUnJ7"
      },
      "outputs": [],
      "source": [
        "# verification eval\n",
        "pred_id_strings = eval_verification(unknown_dev_images, known_images, model, similarity_metric, config['batch_size'], mode='val')\n",
        "# verification test\n",
        "pred_id_strings = eval_verification(unknown_test_images, known_images, model, similarity_metric, config['batch_size'], mode='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bCmzKfH7XgG"
      },
      "outputs": [],
      "source": [
        "# add finetune/retrain code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTLW0RPD7XGC"
      },
      "source": [
        "## Generate csv to submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD-r-HmsAeWV"
      },
      "outputs": [],
      "source": [
        "with open(\"verification_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(pred_id_strings)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_id_strings[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPIgq0tMZ8qk"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions submit -c 11-785-f23-hw2p2-verification -f verification_submission.csv -m \"submission\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
